
# 電流源推定の理屈
かなり面倒いので丁寧にはかけません。ここからは線形代数の範囲になります。

これから誤解を恐れずに脳内の活動を推定する方法をを超簡単に言います。
沢山のセンサーと脳の部位がありますが、
こいつらを使って鶴亀算をとくのです。

大学生風に言うと、これは割り算です。
(そもそも鶴亀算とは割り算にほかならないのであります)
一般逆行列を使って割り算して正規化したやつです。
ノイズについてベイズ推定するという要素がありますが、
面倒いのでここでは一般逆行列だけ使って説明します。

まず、脳のソースから出た磁力…電力でも良いですが、
そのようなものがセンサーに届く時、その強さはソースで発生した電力、磁力に比例するはずです。
多分、距離の二乗には反比例すると思います。
つまり…距離云々を除くと簡単な一次連立方程式になるはずなんです。
センサーで捉えた情報を$y$とし、ソースで発生したのを$x$としましょう。
あるソースで発生した電力とセンサーで捉えた電力の関係を下記の式で表せるとします。
$$y=ax$$
これは単純な掛け算なわけですが、1センサー1ソースではなく、多センサー多ソースです。
ここで、沢山になったy,a,xについて、下記のように表すとします。
$$Y={y_1,y_2,y_3......}$$
$$X={x_1,x_2,x_3......}$$
ここで$A$を下記を満たす行列とします。
$$Y=AX$$
この連立方程式を解きます。これがどうしても連立方程式に見えない人は、
行列代数の入門書でも読んで下さい。ネットで検索するよりこれは本の方がいいです。

では解きま…じつは解けません。
一時連立方程式はあんまり数が多くなると解けなくなるのです。
ですが、最も真実に近いっぽいのを推定することは出来ます。
今回、脳全体のある一瞬の波が最も小さくなるような波を推定しましょう。
MinimumNormEstimation(MNE)と言います。
(他に一つ一つの波が一番小さくなるbeamformer法とか色々あります)
導出は有名なムーアペンローズの逆行列とよく似ています…というか、
ムーアペンローズの逆行列を正規化したものなんですけどね。
だから、それを勉強すれば理解が早まると思います。

つまり、条件$Y=AX$のもとで$X$を小さくしたい[^norm]のです。
$X$が小さいってことは下記の式が小さいってことです。
$$||X||^2$$

[^norm]: ノルムが小さいということ。中学生風に言うと絶対値。

まず、ラグランジュの未定乗数法という公式を使います。[^MP]

[^MP]: ムーアペンローズの逆行列ではよくあること。

$f(x,y)$が極値になるx,yは

$$L(x,y,\lambda)=f(x,y)-\lambda g(x,y)$$
の時に

$$\frac{\partial L}{\partial \lambda}= \frac{\partial L}{\partial y}= \frac{\partial L}{\partial x}=0$$
の解、または
$$\frac{\partial g}{\partial x}= \frac{\partial g}{\partial y}$$
の解。…という感じの公式です。
行列の微分をしないといけないので、行列の微分の仕方を確認しておきます。
$$\frac{\partial a^tx}{\partial x}=a$$
$$\frac{\partial x^ta}{\partial x}=a$$

今回はこれを微分します。
$$L=||X||^{2}-\lambda ||Y-AX||^{2}$$

$$L = X^{T}X - \lambda (Y - AX)^{T}(Y - AX)$$
$$= X^{T}X - \lambda (Y^{T} - X^{T}A^{T})(Y - AX)$$
$$= X^{T}X - \lambda (Y^{T}Y - X^{T}A^{T}Y - Y^{T}AX + X^{T}A^{T}AX)$$
$$\frac{\partial L}{\partial X} = 2X - \lambda(- A^{T}Y - A^{T}Y + (A^{T}A + A^{T}A)X)$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{X}{\lambda})$$
これが0になるので
$$(A^{T}A - \frac{I}{\lambda})X = A^{T}Y$$
$$X = (A^{T}A - \frac{I}{\lambda})^{-1}A^{T}Y$$
ここで$\frac{I}{\lambda}$をCとおくと
$$X = (A^{T}A - CI)^{-1}A^{T}Y$$

これで無事$X$を$A$と$Y$で表せました。

ものすごくざっくりというとこの様な事です。
しかし、実はこのままではグニョングニョンな曲線になってしまうのです

## MNEをもっと綺麗に
ムーアペンローズの逆行列は「正しい」のですが、
このような応用数学の世界では「正しい」とはあまりよろしくありません。
なぜなら一つ一つのサンプルに完全にフィットしちゃうと
完全さを求めた無理な曲線になっちゃうのです。
機械学習界隈では過学習といいますね。

ここで、それを何とかするためにどうすればいいか考えてみます。
つまり、完全さを求めるからいけないんです。
「細けえこたあ良いんだよ！」という姿勢で望むことが大事です。
では「細けえこと」とは何でしょうか？
多分「細けえこと」とは「脳の中に想定されすぎている細かすぎる信号」とか
「実際に観測された細かすぎる信号」等でしょう。
そういうちっさすぎる物は無視するか、0に近い数値を掛け算しちゃえば良いのです。

例１：ちっさすぎる数値に0を掛け算する
例２：分散を掛け算する

例２はちょっとわかりにくいでしょうか？
波が大きいということは分散が大きいということであり、
無視できる細かい波は分散が小さいはずです。

そういう０が含まれる行列や分散の行列を、ここでcovariance matrixと呼びます。

$$||X||^2$$
では、仮に$C$として、その上で
$$X^{T}CX$$
が最小になるような条件を設定してあげればいいです。
この$C$は縦と横の長さが同じ正方行列かつ、対角線上以外全部0の行列です。
こういうのを掛け算すると普通に大きさだけ変えられるんですね。
そりゃそうです。行列Iの要素に順番にスカラー値を掛け算していくわけなので、
実際手で確認すれば自明です。

ではそのことを踏まえて今回はこれを微分します。
$$L = X^{T}CX - \lambda ||Y-AX||^{2}$$

$$L = X^{T}CX - \lambda (Y - AX)^{T}(Y - AX)$$
$$= X^{T}CX - \lambda (Y^{T} - X^{T}A^{T})(Y - AX)$$
$$= X^{T}CX - \lambda (Y^{T}Y - X^{T}A^{T}Y - Y^{T}AX + X^{T}A^{T}AX)$$
$$\frac{\partial L}{\partial X} = (C + C^{T})X - \lambda(- A^{T}Y - A^{T}Y + (A^{T}A + A^{T}A)X)$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{(C + C^{T})X}{2\lambda})$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{CX}{\lambda})$$
$$= 2\lambda(A^{T}Y - (A^{T}A + \frac{C}{\lambda})X)$$

これが0になるので
$$(A^{T}A + \frac{C}{\lambda})X = A^{T}Y$$
$$(\lambda A^{T}A + C)X = \lambda A^{T}Y$$
$$X = \lambda (\lambda A^{T}A + C)^{-1}A^{T}Y$$

凄いですね！これやこの、こまけえこたぁ良いんだよ版のMNEの式です。


## MAP推定
今回はラグランジュの未定乗数法で二乗したやり方を書きましたが、
ベイズの視点からMAP推定をするという見方もあります。
しかし、ここまで書いて疲れました。
これはここに書くのが超絶面倒いので書きません。

これの解説だけで本が一冊書けます。

## dSPMの理屈
さて…MNEの式をよく眺めてみましょう。
これは
$$Y=AX$$
の変形であり、シンプルな掛け算なのは言うまでもありません。
そこで、MNEの式を次のように書き直してみます。
$$X=BY$$

ここで、ふとした疑問が出てきます。
Bのノルムが1じゃない場合に奇妙なことが起こります。
Bが1じゃない場合で、空室を撮ったと仮定してみて下さい。
センサーが捉えるノイズと空室のノイズは同じ大きさのはずですが、
Bが1だったら空室がうまく説明できませんね？？？
ということで、これを補正してみます。

$$X'=\frac{BY}{||B||}$$
$$X'=\frac{BY}{\sqrt{BCB^{T}}}$$

このように、ノルムが1になるように割り算してあげることを数学の言葉で
正規化というらしいです。MNEの結果を正規化したものがdSPMです。
これが僕のdSPMに対する理解です。

ところで、分散を1にしてあげる方法もあるのですが、
これを標準化と言います。
かの有名なsLORETAはdSPMに対して正規化ではなく標準化したものです。
式はこうです。
$$X'=\frac{BY}{\sqrt{B}}$$

という感じの理解で多分合ってると思うけど、
間違ってたらごめん(´・ω・｀)
