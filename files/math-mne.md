\newpage

# ソースベース解析の理屈
かなり面倒いので丁寧にはかけません。ここからは線形代数の範囲になります。
分かる人に言うと、疑似逆行列を正規化した方法がMNEです。

以下、二行目で「は？」ってなった人のための解説を書きます。
誤解を恐れずに脳内の活動を推定する方法を超簡単に言いますと、
**ソース推定とは鶴亀算であり、割り算です。**
「鶴亀算と割り算は別のものじゃんｗｗｗ」と思った人は頑張らねばなりません。
なので、これから「は？鶴亀算が割り算なわけないぞ？」と思った初心者向けに書きます。

https://qiita.com/uesseu/items/750c236bfa706c361b3b


MNEはそんなふうに割り算をしてあげた上で、答えをもうちょっと
スムーズにしてあげる為に正規化という処理を施して上げるものです。

つまり、**MNEとはスムージング入りの割り算である！**
と初心者には伝えたいです(怒られそう…)

## もう一寸ちゃんと

まず、脳のソースから出た磁力…電力でも良いですが、そのようなものがセンサーに届く時、
その強さはソースで発生した電力、磁力に比例するはずです。
詳細はアレですね…マクスウェルの方程式ですね。多分、距離の二乗には反比例すると思います。
つまり…距離云々を除くと簡単な一次連立方程式になるはずなんです。
センサーで捉えた情報を$y$とし、ソースで発生したのを$x$としましょう。
あるソースで発生した電力とセンサーで捉えた電力の関係を下記の式で表せるとします。
$$y=ax$$
これは単純な掛け算なわけですが、1センサー1ソースではなく、多センサー多ソースです。
ここで、沢山になったy,a,xについて、下記のように表すとします。
$$Y={y_1,y_2,y_3......}$$
$$X={x_1,x_2,x_3......}$$
ここで$A$を下記を満たす行列とします。
$$Y=AX$$
この連立方程式を解きます。
横と縦を掛け算するので、行列の掛け算は連立方程式になるのは自明です。
これがどうしても連立方程式に見えない人は、
行列代数の入門書でも読んで下さい。ネットで検索するよりこれは本の方がいいです。

では解きま…じつは解けません。一次連立方程式はあんまり数が多くなると解けなくなるのです。
鶴亀カブトムシ算は無限の解があるのです。

で、最も真実に近いっぽいのを推定していきます。
今回、脳全体のある一瞬の波が最も小さくなるような波を推定しましょう。
何故最小にしたいかって？乱れた波というものはグニョングニョンしてるので
直感的に言ってグニョングニョンしてる波よりも静かな波のほうが
本物っぽいでしょう？と位にしか言えません！

この方法がMinimumNormEstimation(MNE)です。
(他に一つ一つの波が一番小さくなるbeamformer法とか色々あります)
導出は有名なムーアペンローズの逆行列とよく似ています…というか、
ムーアペンローズの逆行列を正規化したものなんですけどね。
だから、それを勉強すれば理解が早まると思います。
ここではムーアペンローズの逆行列について全ては説明しません。

つまり、条件$Y=AX$のもとで$X$を小さくしたい[^norm]のです。
$X$が小さいってことは下記の式が小さいってことです。
$$||X||^2$$

[^norm]: ノルムが小さいということ。中学生風に言うと絶対値。

まず、ラグランジュの未定乗数法という公式を使います。[^MP]

[^MP]: ムーアペンローズの逆行列ではよくあること。

$f(x,y)$が極値になるx,yは

$$L(x,y,\lambda)=f(x,y)-\lambda g(x,y)$$
の時に

$$\frac{\partial L}{\partial \lambda}= \frac{\partial L}{\partial y}= \frac{\partial L}{\partial x}=0$$
の解、または
$$\frac{\partial g}{\partial x}= \frac{\partial g}{\partial y}$$
の解。…という感じの公式です。
行列の微分をしないといけないので、行列の微分の仕方を確認しておきます。
$$\frac{\partial a^tx}{\partial x}=a$$
$$\frac{\partial x^ta}{\partial x}=a$$

今回はこれを微分します。
$$L=||X||^{2}-\lambda ||Y-AX||^{2}$$

$$L = X^{T}X - \lambda (Y - AX)^{T}(Y - AX)$$
$$= X^{T}X - \lambda (Y^{T} - X^{T}A^{T})(Y - AX)$$
$$= X^{T}X - \lambda (Y^{T}Y - X^{T}A^{T}Y - Y^{T}AX + X^{T}A^{T}AX)$$
$$\frac{\partial L}{\partial X} = 2X - \lambda(- A^{T}Y - A^{T}Y + (A^{T}A + A^{T}A)X)$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{X}{\lambda})$$
これが0になるので
$$(A^{T}A - \frac{I}{\lambda})X = A^{T}Y$$
$$X = (A^{T}A - \frac{I}{\lambda})^{-1}A^{T}Y$$
ここで$\frac{I}{\lambda}$をCとおくと
$$X = (A^{T}A - CI)^{-1}A^{T}Y$$

これで無事$X$を$A$と$Y$で表せました。

ものすごくざっくりというとこの様な事です。
しかし、実はこのままではグニョングニョンな曲線になってしまうのです

## ムーアペンローズをもっと綺麗に
ムーアペンローズの逆行列は「正しい」のですが、
応用数学の世界では「正しい」はあまりよろしくありません。
なぜなら一つ一つのサンプルに完全にフィットしちゃうと
完全さを求めた無理な曲線になっちゃうのです。機械学習界隈では過学習といいますね。

ここで、それを何とかするためにどうすればいいか考えてみます。
つまり、完全さを求めるからいけないんです。
「細けえこたあ良いんだよ！」という姿勢で望むことが大事です。
では「細けえこと」とは何でしょうか？
多分「細けえこと」とは「脳の中に想定されすぎている細かすぎる信号」とか
「実際に観測された細かすぎる信号」等でしょう。
そういうちっさすぎる物は無視するか、0に近い数値を掛け算しちゃえば良いのです。

例１：ちっさすぎる数値に0を掛け算する
例２：分散を掛け算する

例２はちょっとわかりにくいでしょうか？
波が大きいということは分散が大きいということであり、無視できる細かい波は分散が小さいはずです。
そういう０が含まれる行列や分散の行列を、ここでcovariance matrixと呼びます。

$$||X||^2$$
では、仮に$C$として、その上で
$$X^{T}CX$$
が最小になるような条件を設定してあげればいいです。
この$C$は縦と横の長さが同じ正方行列かつ、対角線上以外全部0の行列です。
こういうのを掛け算すると普通に大きさだけ変えられるんですね。
そりゃそうです。行列Iの要素に順番にスカラー値を掛け算していくわけなので、
実際手で確認すれば自明です。

ではそのことを踏まえて今回はこれを微分します。
$$L = X^{T}CX - \lambda ||Y-AX||^{2}$$

$$L = X^{T}CX - \lambda (Y - AX)^{T}(Y - AX)$$
$$= X^{T}CX - \lambda (Y^{T} - X^{T}A^{T})(Y - AX)$$
$$= X^{T}CX - \lambda (Y^{T}Y - X^{T}A^{T}Y - Y^{T}AX + X^{T}A^{T}AX)$$
$$\frac{\partial L}{\partial X} = (C + C^{T})X - \lambda(- A^{T}Y - A^{T}Y + (A^{T}A + A^{T}A)X)$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{(C + C^{T})X}{2\lambda})$$
$$= 2\lambda(A^{T}Y - A^{T}AX + \frac{CX}{\lambda})$$
$$= 2\lambda(A^{T}Y - (A^{T}A + \frac{C}{\lambda})X)$$

これが0になるので
$$(A^{T}A + \frac{C}{\lambda})X = A^{T}Y$$
$$(\lambda A^{T}A + C)X = \lambda A^{T}Y$$
$$X = \lambda (\lambda A^{T}A + C)^{-1}A^{T}Y$$

凄いですね！これやこの、こまけえこたぁ良いんだよ版のMNEの式です。


## MAP推定
今回はラグランジュの未定乗数法で二乗したやり方を書きましたが、
ベイズの視点からMAP推定をするという見方もあります。
しかし、ここまで書いて疲れました。これはここに書くのが超絶面倒いので書きません。

これの解説だけで本が一冊書けます。ぷるむるでも読めば良いんじゃないかな？

## dSPMやsLORETAの理屈
さて…MNEの式をよく眺めてみましょう。これは
$$Y=AX$$
の変形であり、シンプルな掛け算なのは言うまでもありません。
そこで、MNEの式を次のように書き直してみます。
$$X=A^\dagger Y$$

ここで、ふとした疑問が出てきます。
$A^\dagger$のノルムが1じゃない場合に奇妙なことが起こります。
$A^\dagger$が1じゃない場合で、空室を撮ったと仮定してみて下さい。
センサーが捉えるノイズと空室のノイズは同じ大きさのはずですが、
$A^\dagger$が1だったら空室がうまく説明できませんね？？？
ということで、これを補正してみます。

$$X'=\frac{A^\dagger Y}{||A^\dagger||}$$
$$X'=\frac{A^\dagger Y}{\sqrt{{A^\dagger C A^\dagger}^{T}}}$$

このように、ノルムが1になるように割り算してあげることを数学の言葉で
正規化というらしいです。MNEの結果を正規化したものがdSPMです。これが僕のdSPMに対する理解です。

ところで、分散を1にしてあげる方法もあるのですが、これを標準化と言います。
かの有名なsLORETAはdSPMに対して正規化ではなく標準化したものです。式はこうです。
$$X'=\frac{A^\dagger Y}{\sqrt{A^\dagger}}$$

という感じの理解で多分合ってると思うけど、間違ってたらごめん(´・ω・｀)
